\hsection{Search Operators}%
\label{sec:searchOperators}%
%
One of the most important design choices of a metaheuristic optimization algorithm are the search operators employed.%
%
\hsection{Definitions}%
%
\begin{definition}[Search Operator]%
\label{def:searchOp}%
An $k$-ary \emph{search operator}~$\searchOp:\searchSpace^k\times\random\mapsto\searchSpace$ is a left-total function which accepts $k$~points in the search space~$\searchSpace$ (and a source~$\random$ of randomness) as input and returns one point in the search space as output.%
\end{definition}%
%
Special cases of search operators are%
%
\begin{itemize}%
\item nullary operators ($k=0$, see \autoref{lst:op0}) sample a new point from the search space without using any information from an existing points,%
\item unary operators ($k=1$, see \autoref{lst:op1}) sample a new point from the search space based on the information of one existing point, and%
\item binary operators ($k=2$, see \autoref{lst:op2}) sample a new point from the search space by combining information from two existing points.%
\end{itemize}%
%
Operators that take existing points in the search space as input tend to sample new points which, in some sort, are similar to their inputs.
They allow us to define proximity-based relationships over the search space, such as the common concept of neighborhoods.

\begin{definition}[Neighborhood]
A unary operator~$\searchOp:\searchSpace\times\random\mapsto\searchSpace$ defines a \emph{neighborhood} relationship over a search space where a point~$\sespel_1\in\searchSpace$ is called a \emph{neighbor} of a point~$\sespel_2\in\searchSpace$ if and only if~$\sespel_1$ could be the result of an application of~$\searchOp$ to~$\sespel_2$.%
\end{definition}%
%
\endhsection%
%
\hsection{A Programmer's Perspective}%
%
If we look at this from the perspective of the programmer, then a search operator is basically an object with a function that accepts a random number generator and $k$~existing points in the search space~\searchSpace.
Since our search space data structures are re-useable containers such as \codeils{list} or \numpyndarrays, we also pass in one such container as destination to receive the newly sampled point.
We can thus define a few very simple API for search operators as components:%
%
\moptipyCode{moptipy/api/operators.py}{--labels op0 --args doc}{op0}{A base class for nullary search operators.}%
%
\moptipyCode{moptipy/api/operators.py}{--labels op1 --args doc}{op1}{A base class for unary search operators.}%
%
\moptipyCode{moptipy/api/operators.py}{--labels op2 --args doc}{op2}{A base class for binary search operators.}%
%
Whether, which, and how such such operators are used depends on the nature of the optimization algorithms and will be discussed later on.

Search operators are often \emph{randomized}, which means invoking the same operator with the same input multiple times may yield different results.
In the definitions, this is signified by the component~$\random$ in their input.
Therefore \autoref{lst:op0}, \ref{lst:op1}, and \ref{lst:op2} all expect an instance of \numpyGenerator, a pseudorandom number generator of the \numpy\ library, as parameter.
\endhsection%
%
\hsection{Example: Job Shop Scheduling}%
%
We will step-by-step introduce the concepts of nullary, unary, and binary search operators in the subsections of TODO on metaheuristics as they come.
This makes more sense from a didactic perspective.
However, we can guess what search operators would probably do in this context:%
%
\begin{enumerate}%
\item a nullary operator would randomly create a permutation with repetition where each of the values in~\intRange{0}{\jsspJobs-1} occurs exactly \jsspMachines~times.%
\item a unary operator would accept such a permutation with repetitions and create a somewhat modified copy of it (that still obeys the validity constraint above), and%
\item a binary operator would accept two and try to merge them in some reasonable (and yet, still somewhat random) way.%
\end{enumerate}%
%
As to why and how we will do that {\dots} that we will learn later.%
\endhsection%
%
\hsection{Summary}%
There is not much to sum up here.
We will really dig into the concept of search operators when we explore metaheuristic optimization methods.
However, at this point, the keen reader may already anticipate some of the concepts that we learn later.
On one side, we do have a space~\solutionSpace\ of solutions~\solspel\ and an objective function~\objf\ that rates their quality.
In the case of our \gls{JSSP} example, we chose Gantt charts as~\solutionSpace, which are not trivial to deal with.
Therefore, on the other side, we defined a much simpler search space~\searchSpace\ and a decoding function~\decode\ that can map one element~$\sespel\in\searchSpace$ to an element~$\solspel\in\solutionSpace$.
One main ingredient that seems to be missing before we can actually tackle an optimization problem are a method to sample points in~\searchSpace\ and to maybe navigate from one existing such point~$\sespel_1$ to another one~$\sespel_2$.
This is what search operators can do.
And \emph{how} they do it, as said, will be explored a bit later.%
\endhsection%
\endhsection%
%
